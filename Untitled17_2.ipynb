{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vTFAX_xdPy9J"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import random\n",
        "import numpy as np\n",
        "\n",
        "# Set random seed for reproducibility\n",
        "random.seed(42)\n",
        "np.random.seed(42)\n",
        "\n",
        "# Function to generate rounded off numbers\n",
        "def generate_rounded_off_numbers():\n",
        "    num = round(random.uniform(1, 10000), 2)\n",
        "    rounded = round(num)\n",
        "    return f\"{num}\", f\"{rounded}\"\n",
        "\n",
        "# Function to generate decimal precision differences\n",
        "def generate_decimal_precision_difference():\n",
        "    num = round(random.uniform(1, 10000), random.randint(1, 5))\n",
        "    diff_precision = f\"{num:.{random.randint(1, 5)}f}\"\n",
        "    return f\"{num}\", f\"{diff_precision}\"\n",
        "\n",
        "# Function to generate leading zero issues correctly\n",
        "def generate_leading_zero_issue():\n",
        "    num = random.randint(1, 999999)\n",
        "    leading_zero = f\"{num:08d}\"  # Ensuring at least 8-digit leading zeros\n",
        "    return leading_zero, str(num)\n",
        "\n",
        "# Function to generate thousands separator differences\n",
        "def generate_thousands_separator_difference():\n",
        "    num = random.randint(1000, 9999999)\n",
        "    with_separator = f\"{num:,}\"\n",
        "    return with_separator, str(num)\n",
        "\n",
        "# Function to generate negative vs positive numbers\n",
        "def generate_negative_vs_positive():\n",
        "    num = random.randint(1, 10000)\n",
        "    return f\"{-num}\", f\"{num}\"\n",
        "\n",
        "# Function to generate scientific notation differences with a broader range\n",
        "def generate_scientific_notation_difference():\n",
        "    num = random.uniform(1e-8, 1e8)  # Very small and very large numbers\n",
        "    sci_notation = \"{:.4E}\".format(num)  # More decimal places for better distinction\n",
        "    return f\"{num}\", sci_notation\n",
        "\n",
        "# Function to generate currency symbol differences with proper encoding and notation handling\n",
        "def generate_currency_symbol_difference():\n",
        "    currency_symbols = ['$', 'â‚¬', 'Â£', 'Â¥', 'â‚¹', 'â‚½', 'â‚©', 'â‚º', 'â‚´', 'â‚¦']\n",
        "    num = round(random.uniform(1, 10000), 2)\n",
        "    symbol = random.choice(currency_symbols)\n",
        "    with_symbol = f\"{symbol}{num:,.2f}\".replace(\",\", \"\")  # Remove comma to prevent formatting errors\n",
        "    with_code = f\"{num:.2f} {symbol}\"\n",
        "    return with_symbol, with_code\n",
        "\n",
        "# Number of records per category\n",
        "num_records = 1000\n",
        "\n",
        "# Generate datasets for each category\n",
        "data_categories = {\n",
        "    \"Rounded Off Numbers\": [generate_rounded_off_numbers() for _ in range(num_records)],\n",
        "    \"Decimal Precision Difference\": [generate_decimal_precision_difference() for _ in range(num_records)],\n",
        "    \"Leading Zero Issue\": [generate_leading_zero_issue() for _ in range(num_records)],\n",
        "    \"Thousands Separator Difference\": [generate_thousands_separator_difference() for _ in range(num_records)],\n",
        "    \"Negative vs Positive\": [generate_negative_vs_positive() for _ in range(num_records)],\n",
        "    \"Scientific Notation Difference\": [generate_scientific_notation_difference() for _ in range(num_records)],\n",
        "    \"Currency Symbol Difference\": [generate_currency_symbol_difference() for _ in range(num_records)],\n",
        "}\n",
        "\n",
        "# Convert to DataFrame format\n",
        "df_list = []\n",
        "for category, records in data_categories.items():\n",
        "    df_temp = pd.DataFrame(records, columns=[\"Source\", \"Destination\"])\n",
        "    df_temp[\"Label\"] = category\n",
        "    df_list.append(df_temp)\n",
        "\n",
        "# Combine all categories into a single DataFrame\n",
        "df_number_discrepancies = pd.concat(df_list, ignore_index=True)\n",
        "\n",
        "# Save the dataset to a CSV file with UTF-8 encoding to avoid corruption\n",
        "file_path = \"Number_Based_Discrepancies_Fixed.csv\"\n",
        "df_number_discrepancies.to_csv(file_path, index=False, encoding=\"utf-8\")\n",
        "\n",
        "print(f\"Dataset has been saved to {file_path}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install rapidfuzz"
      ],
      "metadata": {
        "id": "c6m_AHV0KcGo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import col, udf, when\n",
        "from pyspark.sql.types import ArrayType, DoubleType, StringType\n",
        "import re\n",
        "from rapidfuzz import fuzz"
      ],
      "metadata": {
        "id": "Qf2-m7AbKgYg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import col, udf\n",
        "from pyspark.sql.types import ArrayType, IntegerType\n",
        "import re\n",
        "\n",
        "# Initialize Spark Session\n",
        "spark = SparkSession.builder.appName(\"NumberDiscrepancyAnalysis\").getOrCreate()\n",
        "\n",
        "# Define function to apply different logics for each discrepancy type\n",
        "def compute_discrepancy_scores(label, source, destination):\n",
        "    \"\"\"\n",
        "    Assigns a score based on the discrepancy type.\n",
        "    Each type has a custom logic to determine whether the discrepancy is valid.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Remove non-numeric characters for comparison (except negatives & decimals)\n",
        "        source_clean = re.sub(r\"[^\\d\\.\\-E]\", \"\", source)\n",
        "        destination_clean = re.sub(r\"[^\\d\\.\\-E]\", \"\", destination)\n",
        "\n",
        "        try:\n",
        "            source_num = float(source_clean)\n",
        "            destination_num = float(destination_clean)\n",
        "        except ValueError:\n",
        "            return [0] * 7  # If conversion fails, return zero for all categories\n",
        "\n",
        "        # Initialize all scores with 0\n",
        "        scores = [0] * 7\n",
        "\n",
        "        # Apply logic for each category\n",
        "        if label == \"Rounded Off Numbers\":\n",
        "            if \"E\" not in source and \"E\" not in destination:\n",
        "              scores[0] = 100 if abs(source_num - destination_num) < 1 else 0\n",
        "\n",
        "        elif label == \"Decimal Precision Difference\":\n",
        "            source_str = \"{:.10f}\".format(source_num).rstrip('0')  # Convert to string and remove trailing zeros\n",
        "            destination_str = \"{:.10f}\".format(destination_num).rstrip('0')\n",
        "            scores[1] = 100 if  int(float(source_str)) == int(float(destination_str)) else 0\n",
        "\n",
        "        elif label == \"Leading Zero Issue\":\n",
        "            scores[2] = 100 if source_clean.lstrip(\"0\") == destination_clean.lstrip(\"0\") else 0\n",
        "\n",
        "        elif label == \"Thousands Separator Difference\":\n",
        "            scores[3] = 100 if source_clean.replace(\",\", \"\") == destination_clean.replace(\",\", \"\") else 0\n",
        "\n",
        "        elif label == \"Negative vs Positive\":\n",
        "            scores[4] = 100 if abs(source_num) == abs(destination_num) and source_num != destination_num else 0\n",
        "\n",
        "        elif label == \"Scientific Notation Difference\":\n",
        "          scores[5] = 100 if abs(source_num - destination_num) < 5 else 0\n",
        "        elif label == \"Currency Symbol Difference\":\n",
        "            scores[6] = 100 if source_clean == destination_clean else 0\n",
        "\n",
        "        return scores\n",
        "\n",
        "    except Exception:\n",
        "        return [0] * 7  # Default low scores if an error occurs\n",
        "\n",
        "# Register UDF in Spark\n",
        "discrepancy_udf = udf(lambda label, src, dst: compute_discrepancy_scores(label, src, dst), ArrayType(IntegerType()))\n",
        "\n",
        "# Load dataset\n",
        "file_path = \"/content/Number_Based_Discrepancies_Fixed.csv\"\n",
        "df_spark = spark.read.csv(file_path, header=True, inferSchema=True)\n",
        "\n",
        "# Apply the discrepancy detection function\n",
        "df_spark = df_spark.withColumn(\"Discrepancy Scores\", discrepancy_udf(col(\"Label\"), col(\"Source\"), col(\"Destination\")))\n",
        "\n",
        "# Extract individual discrepancy columns\n",
        "df_spark = df_spark.withColumn(\"Rounded Off Score\", col(\"Discrepancy Scores\")[0]) \\\n",
        "                   .withColumn(\"Decimal Precision Score\", col(\"Discrepancy Scores\")[1]) \\\n",
        "                   .withColumn(\"Leading Zero Score\", col(\"Discrepancy Scores\")[2]) \\\n",
        "                   .withColumn(\"Thousands Separator Score\", col(\"Discrepancy Scores\")[3]) \\\n",
        "                   .withColumn(\"Negative vs Positive Score\", col(\"Discrepancy Scores\")[4]) \\\n",
        "                   .withColumn(\"Scientific Notation Score\", col(\"Discrepancy Scores\")[5]) \\\n",
        "                   .withColumn(\"Currency Symbol Score\", col(\"Discrepancy Scores\")[6]) \\\n",
        "                   .drop(\"Discrepancy Scores\")\n",
        "\n",
        "# Save the processed dataset in CSV format\n",
        "csv_output_path = \"Scored_Number_Based_Discrepancies.csv\"\n",
        "df_spark.toPandas().to_csv(csv_output_path, index=False, encoding=\"utf-8\")\n",
        "\n",
        "print(f\"Scored dataset saved to: {csv_output_path}\")\n"
      ],
      "metadata": {
        "id": "mF_YXoLDKXG5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import col, udf\n",
        "from pyspark.sql.types import ArrayType, IntegerType\n",
        "import re\n",
        "import math\n",
        "from decimal import Decimal\n",
        "\n",
        "# Initialize Spark Session\n",
        "spark = SparkSession.builder.appName(\"AutomaticNumberDiscrepancyAnalysis\").getOrCreate()\n",
        "\n",
        "# Define function to automatically detect discrepancies\n",
        "def detect_number_discrepancies(source, destination):\n",
        "    try:\n",
        "        # Remove non-numeric characters (except negatives, decimals, 'E' for scientific notation)\n",
        "        source_clean = re.sub(r\"[^\\d\\.\\-E]\", \"\", source)\n",
        "        destination_clean = re.sub(r\"[^\\d\\.\\-E]\", \"\", destination)\n",
        "\n",
        "        try:\n",
        "            source_num = float(source_clean)\n",
        "            destination_num = float(destination_clean)\n",
        "        except ValueError:\n",
        "            return [0] * 7  # If conversion fails, return zero for all categories\n",
        "\n",
        "        # Initialize all scores to 0\n",
        "        scores = [0] * 7\n",
        "\n",
        "        # 1. Rounded Off Numbers\n",
        "        if \"E\" not in source and \"E\" not in destination:\n",
        "            scores[0] = 100 if abs(source_num - destination_num) < 1 else 0\n",
        "\n",
        "        # 2. Decimal Precision Difference\n",
        "        source_str = str(Decimal(source_num).normalize())  # Normalize decimal representation\n",
        "        destination_str = str(Decimal(destination_num).normalize())\n",
        "        if int(float(source_str)) == int(float(destination_str)):  # Same integer part\n",
        "            precision_difference = abs(len(source_str.split('.')[1]) - len(destination_str.split('.')[1])) \\\n",
        "                if '.' in source_str and '.' in destination_str else 0\n",
        "            scores[1] = max(0, 100 - precision_difference * 10)  # Deduct 10 points per extra decimal\n",
        "\n",
        "        # 3. Leading Zero Issue\n",
        "        scores[2] = 100 if source_clean.lstrip(\"0\") == destination_clean.lstrip(\"0\") else 0\n",
        "\n",
        "        # 4. Thousands Separator Difference\n",
        "        scores[3] = 100 if source_clean.replace(\",\", \"\").replace(\" \", \"\") == destination_clean.replace(\",\", \"\").replace(\" \", \"\") else 0\n",
        "\n",
        "        # 5. Negative vs Positive\n",
        "        if source_num != 0 and destination_num != 0:  # Exclude zero cases\n",
        "            if math.isclose(abs(source_num), abs(destination_num), rel_tol=1e-9) and source_num != destination_num:\n",
        "                scores[4] = 100\n",
        "\n",
        "        # 6. Scientific Notation Difference\n",
        "        if abs(source_num - destination_num) < 5:  # Allow small differences due to floating point representation\n",
        "            scores[5] = 100\n",
        "\n",
        "        # 7. Currency Symbol Difference (if the numeric value is identical)\n",
        "        scores[6] = 100 if source_clean == destination_clean else 0\n",
        "\n",
        "        return scores\n",
        "\n",
        "    except Exception:\n",
        "        return [0] * 7  # Default low scores if an error occurs\n",
        "\n",
        "# Register UDF in Spark\n",
        "discrepancy_udf = udf(lambda src, dst: detect_number_discrepancies(src, dst), ArrayType(IntegerType()))\n",
        "\n",
        "# Load dataset\n",
        "file_path = \"/content/Number_Based_Discrepancies_Fixed.csv\"\n",
        "df_spark = spark.read.csv(file_path, header=True, inferSchema=True)\n",
        "\n",
        "# Apply the automatic discrepancy detection function\n",
        "df_spark = df_spark.withColumn(\"Discrepancy Scores\", discrepancy_udf(col(\"Source\"), col(\"Destination\")))\n",
        "\n",
        "# Extract individual discrepancy columns\n",
        "df_spark = df_spark.withColumn(\"Rounded Off Score\", col(\"Discrepancy Scores\")[0]) \\\n",
        "                   .withColumn(\"Decimal Precision Score\", col(\"Discrepancy Scores\")[1]) \\\n",
        "                   .withColumn(\"Leading Zero Score\", col(\"Discrepancy Scores\")[2]) \\\n",
        "                   .withColumn(\"Thousands Separator Score\", col(\"Discrepancy Scores\")[3]) \\\n",
        "                   .withColumn(\"Negative vs Positive Score\", col(\"Discrepancy Scores\")[4]) \\\n",
        "                   .withColumn(\"Scientific Notation Score\", col(\"Discrepancy Scores\")[5]) \\\n",
        "                   .withColumn(\"Currency Symbol Score\", col(\"Discrepancy Scores\")[6]) \\\n",
        "                   .drop(\"Discrepancy Scores\")\n",
        "\n",
        "# Save the processed dataset in CSV format\n",
        "csv_output_path = \"Scored_Number_Based_Discrepancies.csv\"\n",
        "df_spark.toPandas().to_csv(csv_output_path, index=False, encoding=\"utf-8\")\n",
        "\n",
        "print(f\"Scored dataset saved to: {csv_output_path}\")\n"
      ],
      "metadata": {
        "id": "linw_9hzt8Pe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import col, udf\n",
        "from pyspark.sql.types import IntegerType\n",
        "import re\n",
        "import math\n",
        "from decimal import Decimal\n",
        "# Initialize Spark Session\n",
        "spark = SparkSession.builder.appName(\"AutomaticNumberDiscrepancyAnalysis\").getOrCreate()\n",
        "# Define function to automatically detect the most relevant discrepancy\n",
        "def detect_primary_discrepancy(source, destination):\n",
        "    \"\"\"\n",
        "    Detects the most relevant discrepancy and assigns a score to only one category.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Remove non-numeric characters (except negatives, decimals, 'E' for scientific notation)\n",
        "        source_clean = re.sub(r\"[^\\d\\.\\-E]\", \"\", source)\n",
        "        destination_clean = re.sub(r\"[^\\d\\.\\-E]\", \"\", destination)\n",
        "        try:\n",
        "            source_num = float(source_clean)\n",
        "            destination_num = float(destination_clean)\n",
        "        except ValueError:\n",
        "            return [0] * 7  # If conversion fails, return zero for all categories\n",
        "        # Initialize all scores to 0\n",
        "        scores = [0] * 7\n",
        "        # Hierarchical priority order for discrepancy detection\n",
        "        if \"E\" not in source and \"E\" not in destination and abs(source_num - destination_num) < 1:\n",
        "            scores[0] = 100  # Rounded Off Numbers\n",
        "        elif int(float(source_clean)) == int(float(destination_clean)):\n",
        "            scores[1] = 100  # Decimal Precision Difference\n",
        "        elif source_clean.lstrip(\"0\") == destination_clean.lstrip(\"0\"):\n",
        "            scores[2] = 100  # Leading Zero Issue\n",
        "        elif source_clean.replace(\",\", \"\").replace(\" \", \"\") == destination_clean.replace(\",\", \"\").replace(\" \", \"\"):\n",
        "            scores[3] = 100  # Thousands Separator Difference\n",
        "        elif source_num != 0 and destination_num != 0 and math.isclose(abs(source_num), abs(destination_num), rel_tol=1e-9) and source_num != destination_num:\n",
        "            scores[4] = 100  # Negative vs Positive\n",
        "        elif abs(source_num - destination_num) < 5:\n",
        "            scores[5] = 100  # Scientific Notation Difference\n",
        "        elif source_clean == destination_clean:\n",
        "            scores[6] = 100  # Currency Symbol Difference\n",
        "        return scores\n",
        "    except Exception:\n",
        "        return [0] * 7  # Default low scores if an error occurs\n",
        "# Register UDF in Spark\n",
        "discrepancy_udf = udf(lambda src, dst: detect_primary_discrepancy(src, dst), ArrayType(IntegerType()))\n",
        "# Load dataset\n",
        "file_path = \"/content/Number_Based_Discrepancies_Fixed.csv\"\n",
        "df_spark = spark.read.csv(file_path, header=True, inferSchema=True)\n",
        "# Apply the automatic discrepancy detection function\n",
        "df_spark = df_spark.withColumn(\"Discrepancy Scores\", discrepancy_udf(col(\"Source\"), col(\"Destination\")))\n",
        "# Extract individual discrepancy columns\n",
        "df_spark = df_spark.withColumn(\"Rounded Off Score\", col(\"Discrepancy Scores\")[0]) \\\n",
        "                   .withColumn(\"Decimal Precision Score\", col(\"Discrepancy Scores\")[1]) \\\n",
        "                   .withColumn(\"Leading Zero Score\", col(\"Discrepancy Scores\")[2]) \\\n",
        "                   .withColumn(\"Thousands Separator Score\", col(\"Discrepancy Scores\")[3]) \\\n",
        "                   .withColumn(\"Negative vs Positive Score\", col(\"Discrepancy Scores\")[4]) \\\n",
        "                   .withColumn(\"Scientific Notation Score\", col(\"Discrepancy Scores\")[5]) \\\n",
        "                   .withColumn(\"Currency Symbol Score\", col(\"Discrepancy Scores\")[6]) \\\n",
        "                   .drop(\"Discrepancy Scores\")\n",
        "# Save the processed dataset in CSV format\n",
        "csv_output_path = \"Scored_Number_Based_Discrepancies.csv\"\n",
        "df_spark.toPandas().to_csv(csv_output_path, index=False, encoding=\"utf-8\")\n",
        "print(f\"Scored dataset saved to: {csv_output_path}\")\n"
      ],
      "metadata": {
        "id": "GNz-9ncWt8We"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.ml.feature import StringIndexer, VectorAssembler\n",
        "from pyspark.ml.classification import RandomForestClassifier\n",
        "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
        "\n",
        "# Start Spark Session\n",
        "spark = SparkSession.builder.appName(\"MixedDataClassification\").getOrCreate()\n",
        "\n",
        "# Load Data\n",
        "df = spark.read.csv(\"/content/Number_Based_Discrepancies_Fixed.csv\", header=True, inferSchema=True)\n"
      ],
      "metadata": {
        "id": "fDGP8yC2t8Zf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Identify Categorical & Numeric Features\n",
        "categorical_columns = [col for col, dtype in df.dtypes if dtype == 'string']\n",
        "numeric_columns = [col for col, dtype in df.dtypes if dtype in ['int', 'double']]\n",
        "\n",
        "# Encode Categorical Features\n",
        "indexers = [StringIndexer(inputCol=col, outputCol=col+\"_indexed\").fit(df) for col in categorical_columns]\n",
        "for indexer in indexers:\n",
        "    df = indexer.transform(df)\n"
      ],
      "metadata": {
        "id": "kdbIA8dSt8cZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Assemble Features\n",
        "feature_columns = [col+\"_indexed\" for col in categorical_columns] + numeric_columns\n",
        "assembler = VectorAssembler(inputCols=feature_columns, outputCol=\"features\")\n",
        "df = assembler.transform(df)"
      ],
      "metadata": {
        "id": "ZiewXXpd17E-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.show()"
      ],
      "metadata": {
        "id": "aFTySw8g186V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.classification import DecisionTreeClassifier"
      ],
      "metadata": {
        "id": "tScFpntW2bO0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Encode Target Variable\n",
        "label_indexer = StringIndexer(inputCol=\"Label\", outputCol=\"label\").fit(df)\n",
        "df = label_indexer.transform(df)\n",
        "\n",
        "# Train-Test Split\n",
        "train_df, test_df = df.randomSplit([0.8, 0.2], seed=42)\n",
        "\n",
        "# Train Model (Random Forest)\n",
        "dt = DecisionTreeClassifier(featuresCol=\"features\", labelCol=\"label\", maxBins=7000)\n",
        "model = dt.fit(train_df)\n",
        "\n",
        "# Predict\n",
        "predictions = model.transform(test_df)\n",
        "\n",
        "# Evaluate\n",
        "evaluator = MulticlassClassificationEvaluator(labelCol=\"label\", metricName=\"accuracy\")\n",
        "accuracy = evaluator.evaluate(predictions)\n",
        "print(\"Accuracy:\", accuracy)\n"
      ],
      "metadata": {
        "id": "KJQFuFXyt8fE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install pytorch_tabnet"
      ],
      "metadata": {
        "id": "SJ8wsVZM8pcZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pytorch_tabnet.tab_model import TabNetClassifier\n",
        "import numpy as np\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "# Start PySpark Session\n",
        "spark = SparkSession.builder.appName(\"TabNetRawData\").getOrCreate()\n",
        "\n",
        "# Load Dataset (without conversion)\n",
        "df = spark.read.csv(\"/content/Number_Based_Discrepancies_Fixed.csv\", header=True, inferSchema=True)\n",
        "\n",
        "# Convert Spark DataFrame to Pandas (TabNet doesn't support Spark directly)\n",
        "df_pd = df.toPandas()\n",
        "\n",
        "# Split Data\n",
        "X = df_pd.drop(columns=['Label'])\n",
        "y = df_pd['Label']\n",
        "\n",
        "# Train TabNet Model (directly on raw data)\n",
        "tabnet = TabNetClassifier()\n",
        "tabnet.fit(X.to_numpy(), y.to_numpy(), eval_set=[(X.to_numpy(), y.to_numpy())], max_epochs=10)\n",
        "\n",
        "# Predict and Evaluate\n",
        "y_pred = tabnet.predict(X.to_numpy())\n",
        "accuracy = (y_pred == y).mean()\n",
        "\n",
        "print(\"Accuracy:\", accuracy)\n"
      ],
      "metadata": {
        "id": "B0PSPhwk2afe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import FTTransformer\n",
        "import torch\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load dataset (without preprocessing)\n",
        "df = pd.read_csv(\"/content/Number_Based_Discrepancies_Fixed.csv\")\n",
        "\n",
        "# Separate features and target\n",
        "X = df.drop(columns=['Label'])  # Keeping raw 'Source' & 'Destination'\n",
        "y = df['Label']\n",
        "\n",
        "# Encode target (required for deep learning models)\n",
        "label_encoder = LabelEncoder()\n",
        "y_encoded = label_encoder.fit_transform(y)\n",
        "\n",
        "# Train-Test Split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y_encoded, test_size=0.2, random_state=42)\n",
        "\n",
        "# Convert categorical + numerical features automatically\n",
        "model = FTTransformer(d_numerical=X.shape[1], categories=[X[col].nunique() for col in X.select_dtypes('object').columns])\n",
        "\n",
        "# Train directly on raw data\n",
        "model.fit(X.to_numpy(), y.to_numpy())\n",
        "\n",
        "# Predict & Evaluate\n",
        "y_pred = model.predict(X.to_numpy())\n",
        "accuracy = accuracy_score(y, y_pred)\n",
        "\n",
        "print(\"FT-Transformer Accuracy:\", accuracy)\n"
      ],
      "metadata": {
        "id": "mWA8tq6l-azo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load dataset (no preprocessing)\n",
        "df = pd.read_csv(\"/content/Number_Based_Discrepancies_Fixed.csv\")\n",
        "\n",
        "# Separate features and target\n",
        "X = df.drop(columns=['Label'])  # Keeping raw 'Source' & 'Destination'\n",
        "y = df['Label']\n",
        "\n",
        "# Encode categorical target\n",
        "label_encoder = LabelEncoder()\n",
        "y_encoded = label_encoder.fit_transform(y)\n",
        "\n",
        "# Train-Test Split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y_encoded, test_size=0.2, random_state=42)\n",
        "\n",
        "# Convert to Tensor\n",
        "X_train_tensor = torch.tensor(X_train.to_numpy(), dtype=torch.float32)\n",
        "X_test_tensor = torch.tensor(X_test.to_numpy(), dtype=torch.float32)\n",
        "y_train_tensor = torch.tensor(y_train, dtype=torch.long)\n",
        "y_test_tensor = torch.tensor(y_test, dtype=torch.long)\n",
        "\n",
        "# Define a Simple Neural Network\n",
        "class MLP(nn.Module):\n",
        "    def __init__(self, input_dim, output_dim):\n",
        "        super(MLP, self).__init__()\n",
        "        self.fc1 = nn.Linear(input_dim, 128)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.fc2 = nn.Linear(128, 64)\n",
        "        self.fc3 = nn.Linear(64, output_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.relu(self.fc1(x))\n",
        "        x = self.relu(self.fc2(x))\n",
        "        return self.fc3(x)\n",
        "\n",
        "# Initialize Model\n",
        "input_dim = X_train.shape[1]\n",
        "output_dim = len(label_encoder.classes_)\n",
        "model = MLP(input_dim, output_dim)\n",
        "\n",
        "# Define Loss and Optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
        "\n",
        "# Train Model\n",
        "epochs = 10\n",
        "for epoch in range(epochs):\n",
        "    optimizer.zero_grad()\n",
        "    outputs = model(X_train_tensor)\n",
        "    loss = criterion(outputs, y_train_tensor)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    print(f\"Epoch {epoch+1}/{epochs}, Loss: {loss.item()}\")\n",
        "\n",
        "# Predict\n",
        "with torch.no_grad():\n",
        "    y_pred_tensor = model(X_test_tensor)\n",
        "    y_pred = torch.argmax(y_pred_tensor, axis=1).numpy()\n",
        "\n",
        "# Evaluate\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(\"Neural Network Accuracy:\", accuracy)\n"
      ],
      "metadata": {
        "id": "q_v3_AI--zCE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "IJ_EhD9m_Ohi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load dataset (without preprocessing)\n",
        "df = pd.read_csv(\"/content/Number_Based_Discrepancies_Fixed.csv\")\n",
        "\n",
        "# Separate features and target\n",
        "X = df.drop(columns=['Label'])  # Keeping raw 'Source' & 'Destination'\n",
        "y = df['Label']\n",
        "\n",
        "# Encode categorical target\n",
        "label_encoder = LabelEncoder()\n",
        "y_encoded = label_encoder.fit_transform(y)\n",
        "\n",
        "# Train-Test Split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y_encoded, test_size=0.2, random_state=42)\n",
        "\n",
        "# Convert categorical data inside PyTorch model (instead of preprocessing manually)\n",
        "class MixedDataMLP(nn.Module):\n",
        "    def __init__(self, input_dim, output_dim):\n",
        "        super(MixedDataMLP, self).__init__()\n",
        "        self.embedding = nn.EmbeddingBag(10000, 64, mode=\"mean\")  # Handles categorical text\n",
        "        self.fc1 = nn.Linear(input_dim, 128)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.fc2 = nn.Linear(128, 64)\n",
        "        self.fc3 = nn.Linear(64, output_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.embedding(x.long())  # Convert text/numeric mix to embeddings\n",
        "        x = self.relu(self.fc1(x))\n",
        "        x = self.relu(self.fc2(x))\n",
        "        return self.fc3(x)\n",
        "\n",
        "# Convert Data to Tensors (Handle Text and Numbers Inside Model)\n",
        "X_train_tensor = torch.tensor(X_train.astype(str).applymap(hash).values, dtype=torch.long)\n",
        "X_test_tensor = torch.tensor(X_test.astype(str).applymap(hash).values, dtype=torch.long)\n",
        "y_train_tensor = torch.tensor(y_train, dtype=torch.long)\n",
        "y_test_tensor = torch.tensor(y_test, dtype=torch.long)\n",
        "\n",
        "# Initialize Model\n",
        "input_dim = X_train.shape[1]\n",
        "output_dim = len(label_encoder.classes_)\n",
        "model = MixedDataMLP(input_dim, output_dim)\n",
        "\n",
        "# Define Loss and Optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
        "\n",
        "# Train Model\n",
        "epochs = 10\n",
        "for epoch in range(epochs):\n",
        "    optimizer.zero_grad()\n",
        "    outputs = model(X_train_tensor)\n",
        "    loss = criterion(outputs, y_train_tensor)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    print(f\"Epoch {epoch+1}/{epochs}, Loss: {loss.item()}\")\n",
        "\n",
        "# Predict\n",
        "with torch.no_grad():\n",
        "    y_pred_tensor = model(X_test_tensor)\n",
        "    y_pred = torch.argmax(y_pred_tensor, axis=1).numpy()\n",
        "\n",
        "# Evaluate\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(\"Neural Network Accuracy:\", accuracy)\n"
      ],
      "metadata": {
        "id": "MFOxeQlh_Orl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load dataset (without preprocessing)\n",
        "df = pd.read_csv(\"/content/Number_Based_Discrepancies_Fixed.csv\")\n",
        "\n",
        "# Separate features and target\n",
        "X = df.drop(columns=['Label'])  # Keeping raw 'Source' & 'Destination'\n",
        "y = df['Label']\n",
        "\n",
        "# Encode categorical target\n",
        "label_encoder = LabelEncoder()\n",
        "y_encoded = label_encoder.fit_transform(y)\n",
        "\n",
        "# Train-Test Split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y_encoded, test_size=0.2, random_state=42)\n",
        "\n",
        "# Define Hashing Function (Keeps Values Within Embedding Range)\n",
        "num_buckets = 10000  # Ensure hash values fit within embedding layer\n",
        "def safe_hash(value):\n",
        "    return hash(value) % num_buckets\n",
        "\n",
        "# Convert Data to Tensors Using Safe Hashing\n",
        "X_train_tensor = torch.tensor(X_train.astype(str).map(safe_hash).values, dtype=torch.long)\n",
        "X_test_tensor = torch.tensor(X_test.astype(str).map(safe_hash).values, dtype=torch.long)\n",
        "y_train_tensor = torch.tensor(y_train, dtype=torch.long)\n",
        "y_test_tensor = torch.tensor(y_test, dtype=torch.long)\n",
        "\n",
        "# Define a Model That Accepts Raw Categorical Data\n",
        "class MixedDataMLP(nn.Module):\n",
        "    def __init__(self, input_dim, output_dim):\n",
        "        super(MixedDataMLP, self).__init__()\n",
        "        self.embedding = nn.EmbeddingBag(num_buckets, 64, mode=\"mean\")  # Handles categorical text\n",
        "        self.fc1 = nn.Linear(64, 128)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.fc2 = nn.Linear(128, 64)\n",
        "        self.fc3 = nn.Linear(64, output_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.embedding(x.long())  # Convert text/numeric mix to embeddings\n",
        "        x = self.relu(self.fc1(x))\n",
        "        x = self.relu(self.fc2(x))\n",
        "        return self.fc3(x)\n",
        "\n",
        "# Initialize Model\n",
        "input_dim = X_train.shape[1]\n",
        "output_dim = len(label_encoder.classes_)\n",
        "model = MixedDataMLP(input_dim, output_dim)\n",
        "\n",
        "# Define Loss and Optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
        "\n",
        "# Train Model\n",
        "epochs = 100\n",
        "for epoch in range(epochs):\n",
        "    optimizer.zero_grad()\n",
        "    outputs = model(X_train_tensor)\n",
        "    loss = criterion(outputs, y_train_tensor)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    print(f\"Epoch {epoch+1}/{epochs}, Loss: {loss.item()}\")\n",
        "\n",
        "# Predict\n",
        "with torch.no_grad():\n",
        "    y_pred_tensor = model(X_test_tensor)\n",
        "    y_pred = torch.argmax(y_pred_tensor, axis=1).numpy()\n",
        "\n",
        "# Evaluate\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(\"Neural Network Accuracy:\", accuracy)\n"
      ],
      "metadata": {
        "id": "SSniR_2g_OuH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "lg8JjRSBDw3R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "DPJJkXePDw6I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load dataset (without preprocessing)\n",
        "df = pd.read_csv(\"/content/Number_Based_Discrepancies_Fixed.csv\")\n",
        "\n",
        "# Separate features and target\n",
        "X = df.drop(columns=['Label'])  # Keeping raw 'Source' & 'Destination'\n",
        "y = df['Label']\n",
        "\n",
        "# Encode categorical target\n",
        "label_encoder = LabelEncoder()\n",
        "y_encoded = label_encoder.fit_transform(y)\n",
        "\n",
        "# Train-Test Split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y_encoded, test_size=0.2, random_state=42)\n",
        "\n",
        "# Define Hashing Function (Keeps Values Within Embedding Range)\n",
        "num_buckets = 50000  # Increased bucket size for more unique hashed values\n",
        "def safe_hash(value):\n",
        "    return hash(value) % num_buckets  # Keep values in range [0, num_buckets)\n",
        "\n",
        "# Convert Data to Tensors Using Safe Hashing\n",
        "X_train_tensor = torch.tensor(X_train.astype(str).map(safe_hash).values, dtype=torch.long)\n",
        "X_test_tensor = torch.tensor(X_test.astype(str).map(safe_hash).values, dtype=torch.long)\n",
        "y_train_tensor = torch.tensor(y_train, dtype=torch.long)\n",
        "y_test_tensor = torch.tensor(y_test, dtype=torch.long)\n",
        "\n",
        "# Define an Improved Deep Neural Network\n",
        "class ImprovedMLP(nn.Module):\n",
        "    def __init__(self, input_dim, output_dim):\n",
        "        super(ImprovedMLP, self).__init__()\n",
        "        self.embedding = nn.EmbeddingBag(num_buckets, 128, mode=\"mean\")  # Increased embedding size\n",
        "        self.fc1 = nn.Linear(128, 256)  # More neurons\n",
        "        self.fc2 = nn.Linear(256, 128)\n",
        "        self.fc3 = nn.Linear(128, 64)\n",
        "        self.fc4 = nn.Linear(64, output_dim)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.dropout = nn.Dropout(0.3)  # Dropout layer to prevent overfitting\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.embedding(x.long())  # Hash-based embeddings\n",
        "        x = self.dropout(self.relu(self.fc1(x)))  # Apply dropout\n",
        "        x = self.dropout(self.relu(self.fc2(x)))\n",
        "        x = self.relu(self.fc3(x))\n",
        "        return self.fc4(x)\n",
        "\n",
        "# Initialize Model\n",
        "input_dim = X_train.shape[1]\n",
        "output_dim = len(label_encoder.classes_)\n",
        "model = ImprovedMLP(input_dim, output_dim)\n",
        "\n",
        "# Define Loss and Optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)  # Lower learning rate for stability\n",
        "\n",
        "# Train Model\n",
        "epochs = 200  # More epochs for better learning\n",
        "for epoch in range(epochs):\n",
        "    optimizer.zero_grad()\n",
        "    outputs = model(X_train_tensor)\n",
        "    loss = criterion(outputs, y_train_tensor)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    if epoch % 10 == 0:  # Print loss every 10 epochs\n",
        "        print(f\"Epoch {epoch+1}/{epochs}, Loss: {loss.item()}\")\n",
        "\n",
        "# Predict\n",
        "with torch.no_grad():\n",
        "    y_pred_tensor = model(X_test_tensor)\n",
        "    y_pred = torch.argmax(y_pred_tensor, axis=1).numpy()\n",
        "\n",
        "# Evaluate\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(\"\\nðŸš€ Improved Neural Network Accuracy:\", accuracy)\n"
      ],
      "metadata": {
        "id": "z-Ug_IfFDw85"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load dataset (No preprocessing)\n",
        "df = pd.read_csv(\"/content/Number_Based_Discrepancies_Fixed.csv\")\n",
        "\n",
        "# Separate features and target\n",
        "X = df.drop(columns=['Label'])  # Keep raw 'Source' & 'Destination'\n",
        "y = df['Label']\n",
        "\n",
        "# Encode categorical target\n",
        "label_encoder = LabelEncoder()\n",
        "y_encoded = label_encoder.fit_transform(y)\n",
        "\n",
        "# Train-Test Split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y_encoded, test_size=0.2, random_state=42)\n",
        "\n",
        "# Compute Class Weights (for imbalance handling)\n",
        "class_weights = compute_class_weight(\"balanced\", classes=np.unique(y_encoded), y=y_encoded)\n",
        "class_weights_tensor = torch.tensor(class_weights, dtype=torch.float32)\n",
        "\n",
        "# Convert Data to Tensor (Use categorical encoding)\n",
        "X_train_tensor = torch.tensor(X_train.astype(str).applymap(hash).values % 50000, dtype=torch.long)\n",
        "X_test_tensor = torch.tensor(X_test.astype(str).applymap(hash).values % 50000, dtype=torch.long)\n",
        "y_train_tensor = torch.tensor(y_train, dtype=torch.long)\n",
        "y_test_tensor = torch.tensor(y_test, dtype=torch.long)\n",
        "\n",
        "# Define a More Advanced Neural Network with Learned Embeddings\n",
        "class EmbeddingMLP(nn.Module):\n",
        "    def __init__(self, num_categories, output_dim):\n",
        "        super(EmbeddingMLP, self).__init__()\n",
        "        self.embedding = nn.Embedding(num_categories, 128)  # Learn feature relationships\n",
        "        self.fc1 = nn.Linear(128, 256)\n",
        "        self.bn1 = nn.BatchNorm1d(256)  # Batch normalization\n",
        "        self.fc2 = nn.Linear(256, 128)\n",
        "        self.bn2 = nn.BatchNorm1d(128)\n",
        "        self.fc3 = nn.Linear(128, 64)\n",
        "        self.fc4 = nn.Linear(64, output_dim)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.dropout = nn.Dropout(0.3)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.embedding(x)\n",
        "        x = self.dropout(self.relu(self.bn1(self.fc1(x))))\n",
        "        x = self.dropout(self.relu(self.bn2(self.fc2(x))))\n",
        "        x = self.relu(self.fc3(x))\n",
        "        return self.fc4(x)\n",
        "\n",
        "# Initialize Model\n",
        "num_categories = 50000  # Embedding size\n",
        "output_dim = len(label_encoder.classes_)\n",
        "model = EmbeddingMLP(num_categories, output_dim)\n",
        "\n",
        "# Define Loss (Using Class Weights) and Optimizer\n",
        "criterion = nn.CrossEntropyLoss(weight=class_weights_tensor)\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=0.0005)  # AdamW for better weight decay\n",
        "\n",
        "# Train Model\n",
        "epochs = 300  # Train for longer\n",
        "for epoch in range(epochs):\n",
        "    optimizer.zero_grad()\n",
        "    outputs = model(X_train_tensor)\n",
        "    loss = criterion(outputs, y_train_tensor)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    if epoch % 10 == 0:  # Print every 10 epochs\n",
        "        print(f\"Epoch {epoch+1}/{epochs}, Loss: {loss.item()}\")\n",
        "\n",
        "# Predict\n",
        "with torch.no_grad():\n",
        "    y_pred_tensor = model(X_test_tensor)\n",
        "    y_pred = torch.argmax(y_pred_tensor, axis=1).numpy()\n",
        "\n",
        "# Evaluate\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(\"\\nðŸš€ Final Improved Neural Network Accuracy:\", accuracy)\n"
      ],
      "metadata": {
        "id": "FNCRxBfmDw_r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load dataset (No preprocessing)\n",
        "df = pd.read_csv(\"Number_Based_Discrepancies_Fixed.csv\")\n",
        "\n",
        "# Separate features and target\n",
        "X = df.drop(columns=['Label'])  # Keep raw 'Source' & 'Destination'\n",
        "y = df['Label']\n",
        "\n",
        "# Encode categorical target\n",
        "label_encoder = LabelEncoder()\n",
        "y_encoded = label_encoder.fit_transform(y)\n",
        "\n",
        "# Train-Test Split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y_encoded, test_size=0.2, random_state=42)\n",
        "\n",
        "# Compute Class Weights (for imbalance handling)\n",
        "class_weights = compute_class_weight(\"balanced\", classes=np.unique(y_encoded), y=y_encoded)\n",
        "class_weights_tensor = torch.tensor(class_weights, dtype=torch.float32)\n",
        "\n",
        "# Convert Data to Tensor (Use categorical encoding)\n",
        "num_buckets = 50000  # Embedding size\n",
        "def safe_hash(value):\n",
        "    return hash(value) % num_buckets  # Keep values within embedding range\n",
        "\n",
        "X_train_tensor = torch.tensor(X_train.astype(str).map(safe_hash).values, dtype=torch.long)\n",
        "X_test_tensor = torch.tensor(X_test.astype(str).map(safe_hash).values, dtype=torch.long)\n",
        "y_train_tensor = torch.tensor(y_train, dtype=torch.long)\n",
        "y_test_tensor = torch.tensor(y_test, dtype=torch.long)\n",
        "\n",
        "# Define an Improved Deep Neural Network with Learned Embeddings\n",
        "class EmbeddingMLP(nn.Module):\n",
        "    def __init__(self, num_categories, output_dim):\n",
        "        super(EmbeddingMLP, self).__init__()\n",
        "        self.embedding = nn.Embedding(num_categories, 128)  # Learn feature relationships\n",
        "        self.fc1 = nn.Linear(128 * X_train.shape[1], 256)  # Adjust input size after flattening\n",
        "        self.bn1 = nn.BatchNorm1d(256)  # Batch normalization\n",
        "        self.fc2 = nn.Linear(256, 128)\n",
        "        self.bn2 = nn.BatchNorm1d(128)\n",
        "        self.fc3 = nn.Linear(128, 64)\n",
        "        self.fc4 = nn.Linear(64, output_dim)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.dropout = nn.Dropout(0.3)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.embedding(x).view(x.size(0), -1)  # Flatten embedding output\n",
        "        x = self.dropout(self.relu(self.bn1(self.fc1(x))))\n",
        "        x = self.dropout(self.relu(self.bn2(self.fc2(x))))\n",
        "        x = self.relu(self.fc3(x))\n",
        "        return self.fc4(x)\n",
        "\n",
        "# Initialize Model\n",
        "output_dim = len(label_encoder.classes_)\n",
        "model = EmbeddingMLP(num_buckets, output_dim)\n",
        "\n",
        "# Define Loss (Using Class Weights) and Optimizer\n",
        "criterion = nn.CrossEntropyLoss(weight=class_weights_tensor)\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=0.0005)  # AdamW for better weight decay\n",
        "\n",
        "# Train Model\n",
        "epochs = 300  # Train for longer\n",
        "for epoch in range(epochs):\n",
        "    optimizer.zero_grad()\n",
        "    outputs = model(X_train_tensor)\n",
        "    loss = criterion(outputs, y_train_tensor)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    if epoch % 10 == 0:  # Print every 10 epochs\n",
        "        print(f\"Epoch {epoch+1}/{epochs}, Loss: {loss.item()}\")\n",
        "\n",
        "# Predict\n",
        "with torch.no_grad():\n",
        "    y_pred_tensor = model(X_test_tensor)\n",
        "    y_pred = torch.argmax(y_pred_tensor, axis=1).numpy()\n",
        "\n",
        "# Evaluate\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(\"\\nðŸš€ Final Improved Neural Network Accuracy:\", accuracy)\n"
      ],
      "metadata": {
        "id": "fAMCXDx4DxCy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load dataset (No preprocessing)\n",
        "df = pd.read_csv(\"/content/Number_Based_Discrepancies_Fixed.csv\")\n",
        "\n",
        "# Separate features and target\n",
        "X = df.drop(columns=['Label'])  # Keep raw 'Source' & 'Destination'\n",
        "y = df['Label']\n",
        "\n",
        "# Encode categorical target\n",
        "label_encoder = LabelEncoder()\n",
        "y_encoded = label_encoder.fit_transform(y)\n",
        "\n",
        "# Train-Test Split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y_encoded, test_size=0.2, random_state=42)\n",
        "\n",
        "# Compute Class Weights (for imbalance handling)\n",
        "class_weights = compute_class_weight(\"balanced\", classes=np.unique(y_encoded), y=y_encoded)\n",
        "class_weights_tensor = torch.tensor(class_weights, dtype=torch.float32)\n",
        "\n",
        "# Convert Data to Tensor (Use categorical encoding)\n",
        "num_buckets = 50000  # Embedding size\n",
        "def safe_hash(value):\n",
        "    return hash(value) % num_buckets  # Keep values within embedding range\n",
        "\n",
        "X_train_tensor = torch.tensor(X_train.astype(str).map(safe_hash).values, dtype=torch.long)\n",
        "X_test_tensor = torch.tensor(X_test.astype(str).map(safe_hash).values, dtype=torch.long)\n",
        "y_train_tensor = torch.tensor(y_train, dtype=torch.long)\n",
        "y_test_tensor = torch.tensor(y_test, dtype=torch.long)\n",
        "\n",
        "# Define an Improved Deep Neural Network with Learned Embeddings\n",
        "class EmbeddingMLP(nn.Module):\n",
        "    def __init__(self, num_categories, output_dim):\n",
        "        super(EmbeddingMLP, self).__init__()\n",
        "        self.embedding = nn.Embedding(num_categories, 128)  # Learn feature relationships\n",
        "        self.fc1 = nn.Linear(128 * X_train.shape[1], 256)  # Adjust input size after flattening\n",
        "        self.bn1 = nn.BatchNorm1d(256)  # Batch normalization\n",
        "        self.fc2 = nn.Linear(256, 128)\n",
        "        self.bn2 = nn.BatchNorm1d(128)\n",
        "        self.fc3 = nn.Linear(128, 64)\n",
        "        self.fc4 = nn.Linear(64, output_dim)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.dropout = nn.Dropout(0.3)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.embedding(x).view(x.size(0), -1)  # Flatten embedding output\n",
        "        x = self.dropout(self.relu(self.bn1(self.fc1(x))))\n",
        "        x = self.dropout(self.relu(self.bn2(self.fc2(x))))\n",
        "        x = self.relu(self.fc3(x))\n",
        "        return self.fc4(x)\n",
        "\n",
        "# Initialize Model\n",
        "output_dim = len(label_encoder.classes_)\n",
        "model = EmbeddingMLP(num_buckets, output_dim)\n",
        "\n",
        "# Define Loss (Using Class Weights) and Optimizer\n",
        "criterion = nn.CrossEntropyLoss(weight=class_weights_tensor)\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=0.0005)  # AdamW for better weight decay\n",
        "\n",
        "# Train Model\n",
        "epochs = 300  # Train for longer\n",
        "for epoch in range(epochs):\n",
        "    optimizer.zero_grad()\n",
        "    outputs = model(X_train_tensor)\n",
        "    loss = criterion(outputs, y_train_tensor)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    if epoch % 10 == 0:  # Print every 10 epochs\n",
        "        print(f\"Epoch {epoch+1}/{epochs}, Loss: {loss.item()}\")\n",
        "\n",
        "# Predict\n",
        "with torch.no_grad():\n",
        "    y_pred_tensor = model(X_test_tensor)\n",
        "    y_pred = torch.argmax(y_pred_tensor, axis=1).numpy()\n",
        "\n",
        "# Evaluate\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(\"\\nðŸš€ Final Improved Neural Network Accuracy:\", accuracy)\n"
      ],
      "metadata": {
        "id": "AQvM3516F2db"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load dataset (No preprocessing)\n",
        "df = pd.read_csv(\"/content/Number_Based_Discrepancies_Fixed.csv\")\n",
        "\n",
        "# Separate features and target\n",
        "X = df.drop(columns=['Label'])  # Keep raw 'Source' & 'Destination'\n",
        "y = df['Label']\n",
        "\n",
        "# Encode categorical target\n",
        "label_encoder = LabelEncoder()\n",
        "y_encoded = label_encoder.fit_transform(y)\n",
        "\n",
        "# Train-Test Split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y_encoded, test_size=0.2, random_state=42)\n",
        "\n",
        "# Compute Class Weights (for imbalance handling)\n",
        "class_weights = compute_class_weight(\"balanced\", classes=np.unique(y_encoded), y=y_encoded)\n",
        "class_weights_tensor = torch.tensor(class_weights, dtype=torch.float32)\n",
        "\n",
        "# Convert Data to Tensor (Use categorical encoding)\n",
        "num_buckets = 50000  # Embedding size\n",
        "def safe_hash(value):\n",
        "    return hash(value) % num_buckets  # Keep values within embedding range\n",
        "\n",
        "X_train_tensor = torch.tensor(X_train.astype(str).map(safe_hash).values, dtype=torch.long)\n",
        "X_test_tensor = torch.tensor(X_test.astype(str).map(safe_hash).values, dtype=torch.long)\n",
        "y_train_tensor = torch.tensor(y_train, dtype=torch.long)\n",
        "y_test_tensor = torch.tensor(y_test, dtype=torch.long)\n",
        "\n",
        "# Define a Transformer-Based Model\n",
        "class TransformerModel(nn.Module):\n",
        "    def __init__(self, num_categories, output_dim):\n",
        "        super(TransformerModel, self).__init__()\n",
        "        self.embedding = nn.Embedding(num_categories, 256)  # Larger Embeddings\n",
        "        self.transformer_layer = nn.TransformerEncoderLayer(d_model=256, nhead=8)  # Multi-Head Attention\n",
        "        self.transformer = nn.TransformerEncoder(self.transformer_layer, num_layers=4)\n",
        "        self.fc1 = nn.Linear(256, 128)\n",
        "        self.bn1 = nn.BatchNorm1d(128)\n",
        "        self.fc2 = nn.Linear(128, 64)\n",
        "        self.fc3 = nn.Linear(64, output_dim)\n",
        "        self.leakyrelu = nn.LeakyReLU()\n",
        "        self.dropout = nn.Dropout(0.2)  # Lower dropout for better generalization\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.embedding(x).view(x.size(0), 1, -1)  # Adjust for Transformer\n",
        "        x = self.transformer(x).squeeze(1)  # Pass through Transformer\n",
        "        x = self.leakyrelu(self.bn1(self.fc1(x)))\n",
        "        x = self.leakyrelu(self.fc2(x))\n",
        "        return self.fc3(x)\n",
        "\n",
        "# Initialize Transformer Model\n",
        "output_dim = len(label_encoder.classes_)\n",
        "model = TransformerModel(num_buckets, output_dim)\n",
        "\n",
        "# Define Optimizer & Loss\n",
        "criterion = nn.CrossEntropyLoss(weight=class_weights_tensor)\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=0.001, momentum=0.9)  # Use SGD + Momentum for better generalization\n",
        "\n",
        "# Train Model\n",
        "epochs = 500  # Train longer for Transformers\n",
        "for epoch in range(epochs):\n",
        "    optimizer.zero_grad()\n",
        "    outputs = model(X_train_tensor)\n",
        "    loss = criterion(outputs, y_train_tensor)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    if epoch % 10 == 0:  # Print every 10 epochs\n",
        "        print(f\"Epoch {epoch+1}/{epochs}, Loss: {loss.item()}\")\n",
        "\n",
        "# Predict\n",
        "with torch.no_grad():\n",
        "    y_pred_tensor = model(X_test_tensor)\n",
        "    y_pred = torch.argmax(y_pred_tensor, axis=1).numpy()\n",
        "\n",
        "# Evaluate\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(\"\\nðŸš€ Final Transformer Model Accuracy:\", accuracy)\n"
      ],
      "metadata": {
        "id": "rdCygKfqDxGY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load dataset (No preprocessing)\n",
        "df = pd.read_csv(\"/content/Number_Based_Discrepancies_Fixed.csv\")\n",
        "\n",
        "# Separate features and target\n",
        "X = df.drop(columns=['Label'])  # Keep raw 'Source' & 'Destination'\n",
        "y = df['Label']\n",
        "\n",
        "# Encode categorical target\n",
        "label_encoder = LabelEncoder()\n",
        "y_encoded = label_encoder.fit_transform(y)\n",
        "\n",
        "# Train-Test Split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y_encoded, test_size=0.2, random_state=42)\n",
        "\n",
        "# Compute Class Weights (for imbalance handling)\n",
        "class_weights = compute_class_weight(\"balanced\", classes=np.unique(y_encoded), y=y_encoded)\n",
        "class_weights_tensor = torch.tensor(class_weights, dtype=torch.float32)\n",
        "\n",
        "# Convert Data to Tensor (Use categorical encoding)\n",
        "num_buckets = 50000  # Embedding size\n",
        "def safe_hash(value):\n",
        "    return hash(value) % num_buckets  # Keep values within embedding range\n",
        "\n",
        "X_train_tensor = torch.tensor(X_train.astype(str).map(safe_hash).values, dtype=torch.long)\n",
        "X_test_tensor = torch.tensor(X_test.astype(str).map(safe_hash).values, dtype=torch.long)\n",
        "y_train_tensor = torch.tensor(y_train, dtype=torch.long)\n",
        "y_test_tensor = torch.tensor(y_test, dtype=torch.long)\n",
        "\n",
        "# Define a Transformer-Based Model (FIXED)\n",
        "class TransformerModel(nn.Module):\n",
        "    def __init__(self, num_categories, output_dim):\n",
        "        super(TransformerModel, self).__init__()\n",
        "        self.embedding = nn.Embedding(num_categories, 256)  # Embed categorical features\n",
        "        self.transformer_layer = nn.TransformerEncoderLayer(d_model=256, nhead=8, batch_first=True)  # Use batch_first=True\n",
        "        self.transformer = nn.TransformerEncoder(self.transformer_layer, num_layers=4)\n",
        "        self.fc1 = nn.Linear(256, 128)\n",
        "        self.bn1 = nn.BatchNorm1d(128)\n",
        "        self.fc2 = nn.Linear(128, 64)\n",
        "        self.fc3 = nn.Linear(64, output_dim)\n",
        "        self.leakyrelu = nn.LeakyReLU()\n",
        "        self.dropout = nn.Dropout(0.2)  # Lower dropout for better generalization\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.embedding(x)  # Shape: (batch_size, seq_length, embed_dim)\n",
        "        x = self.transformer(x)  # Transformer expects (batch_size, seq_len, embed_dim)\n",
        "        x = x.mean(dim=1)  # Pool across sequence length\n",
        "        x = self.leakyrelu(self.bn1(self.fc1(x)))\n",
        "        x = self.leakyrelu(self.fc2(x))\n",
        "        return self.fc3(x)\n",
        "\n",
        "# Initialize Transformer Model\n",
        "output_dim = len(label_encoder.classes_)\n",
        "model = TransformerModel(num_buckets, output_dim)\n",
        "\n",
        "# Define Optimizer & Loss\n",
        "criterion = nn.CrossEntropyLoss(weight=class_weights_tensor)\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=0.001, momentum=0.9)  # Use SGD + Momentum for better generalization\n",
        "\n",
        "# Train Model\n",
        "epochs = 500  # Train longer for Transformers\n",
        "for epoch in range(epochs):\n",
        "    optimizer.zero_grad()\n",
        "    outputs = model(X_train_tensor)\n",
        "    loss = criterion(outputs, y_train_tensor)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    if epoch % 10 == 0:  # Print every 10 epochs\n",
        "        print(f\"Epoch {epoch+1}/{epochs}, Loss: {loss.item()}\")\n",
        "\n",
        "# Predict\n",
        "with torch.no_grad():\n",
        "    y_pred_tensor = model(X_test_tensor)\n",
        "    y_pred = torch.argmax(y_pred_tensor, axis=1).numpy()\n",
        "\n",
        "# Evaluate\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(\"\\n Final Transformer Model Accuracy:\", accuracy)\n"
      ],
      "metadata": {
        "id": "MnxYmJP2G7BD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "HzMx-cRCG7Lg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "-iY36NOGG7OQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "tbZJXp84G7Ra"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}