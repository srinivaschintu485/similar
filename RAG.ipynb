{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4c92f3f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3d982b4d",
   "metadata": {},
   "source": [
    "# Notes\n",
    "\n",
    "sentence-transformers is used for generating embeddings from text using models like all-MiniLM-L6-v2, which helps in tasks like semantic search and similarity comparison.\n",
    "\n",
    "faiss-cpu is Facebook's fast similarity search library (CPU version), useful for indexing and retrieving the most relevant vectors efficiently.\n",
    "\n",
    "transformers provides access to Hugging Face models like LLaMA, GPT, etc., for natural language tasks like text generation or question answering.\n",
    "\n",
    "langchain helps orchestrate LLM pipelines by connecting models, prompts, memory, tools, and documents — especially useful in Retrieval-Augmented Generation (RAG) setups.\n",
    "\n",
    "openai allows access to OpenAI’s GPT models like GPT-4 if needed, via API.\n",
    "\n",
    "streamlit is a fast way to build and deploy web apps for your models with a simple Python script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42f10436",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install sentence-transformers faiss-cpu transformers langchain openai streamlit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "572ddadc",
   "metadata": {},
   "outputs": [],
   "source": [
    "schema_chunks = [\n",
    "    \"Table: orders\\nColumns: order_id (INT), customer_id (INT), order_date (DATE), total_amount (FLOAT)\",\n",
    "    \"Table: customers\\nColumns: customer_id (INT), name (TEXT), region (TEXT)\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7e80d627",
   "metadata": {},
   "outputs": [],
   "source": [
    "schema_chunks = [\n",
    "     \"Table: customers\\nColumns: customer_id (INT), name (TEXT), email (TEXT), region (TEXT)\",\n",
    "    \"Table: orders\\nColumns: order_id (INT), customer_id (INT), order_date (DATE), total_amount (FLOAT)\",\n",
    "    \"Table: order_items\\nColumns: order_item_id (INT), order_id (INT), product_id (INT), quantity (INT), item_total (FLOAT)\",\n",
    "    \"Table: products\\nColumns: product_id (INT), product_name (TEXT), category (TEXT), price (FLOAT)\",\n",
    "    \"Table: reviews\\nColumns: review_id (INT), product_id (INT), customer_id (INT), rating (INT), comment (TEXT)\"\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "acbd69e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "import faiss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "cfe827ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import login"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "5415b377",
   "metadata": {},
   "outputs": [],
   "source": [
    "login(\"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\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b4dbd35d",
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_model = SentenceTransformer('all-MiniLM-L6-v2')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddc36fab",
   "metadata": {},
   "source": [
    "# Overview\n",
    "Model Name: sentence-transformers/all-MiniLM-L6-v2\n",
    "\n",
    "Library: Part of the Sentence‑Transformers collection\n",
    "\n",
    "Output Dimension: 384‑dimensional vector per sentence⁚\n",
    "→ Efficient representation of semantic meaning in numeric form \n",
    "License: Apache 2.0 (free and open to all)\n",
    "\n",
    "# Training & Architecture\n",
    "Base model: Built on Microsoft’s MiniLM‑L6‑H384 (uncased)\n",
    "\n",
    "Fine‑tuned with contrastive learning on over 1 billion sentence pairs\n",
    "\n",
    "Trained on TPUs, batch size 1024, learning rate warm-up, using AdamW optimizer\n",
    "\n",
    "\n",
    "# Performance & Features\n",
    "Speed: ~5× faster than all‑mpnet‑base‑v2 on CPU\n",
    "\n",
    "Accuracy: Strong performance on semantic text similarity (e.g., ≈84–85% on STS-B)\n",
    "— all‑mpnet‑base‑v2 is slightly higher (~87–88%) but resource‑heavier\n",
    "\n",
    "Input Limit: Truncates texts longer than 256 tokens by default\n",
    "\n",
    "# Use Cases\n",
    "\n",
    "Ideal for tasks like:\n",
    "\n",
    "Semantic search and retrieval\n",
    "\n",
    "Text clustering (document or sentence level)\n",
    "\n",
    "Paraphrase detection and similarity scoring\n",
    "\n",
    "Lightweight embedding for real-time APIs, mobile apps, or large-scale pipelines\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b3e1d069",
   "metadata": {},
   "outputs": [],
   "source": [
    "schema_vectors = embed_model.encode(schema_chunks)\n",
    "\n",
    "index = faiss.IndexFlatL2(schema_vectors.shape[1]) #he number inside IndexFlatL2(...) is the dimension of each vector, which is usually something like 384 or 768\n",
    "index.add(schema_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e78ca056",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20, 384)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "schema_vectors.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "efe3fd74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.4468237 1.4633219 1.5623817 1.6017404]] [[1 0 4 2]]\n",
      "['Table: orders\\nColumns: order_id (INT), customer_id (INT), order_date (DATE), total_amount (FLOAT)', 'Table: customers\\nColumns: customer_id (INT), name (TEXT), email (TEXT), region (TEXT)', 'Table: reviews\\nColumns: review_id (INT), product_id (INT), customer_id (INT), rating (INT), comment (TEXT)', 'Table: order_items\\nColumns: order_item_id (INT), order_id (INT), product_id (INT), quantity (INT), item_total (FLOAT)']\n"
     ]
    }
   ],
   "source": [
    "user_question = \"For each subscription plan in 2024, identify the top 3 regions with the highest number of active subscribers. For each region, show the average order value, total number of returns, and average product rating across all orders placed during the subscription period. Only consider customers who have not raised any support tickets during their subscription.\"\n",
    "query_vector = embed_model.encode([user_question])\n",
    "\n",
    "D, I = index.search(query_vector, k=4)\n",
    "relevant_chunks = [schema_chunks[i] for i in I[0]]\n",
    "print(D,I)\n",
    "print(relevant_chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f47997d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prompt = f\"\"\"\n",
    "# You are a SQL expert. Based on the schema below, write a valid SQL query.\n",
    "\n",
    "# Schema:\n",
    "# {relevant_chunks[0]}\n",
    "# {relevant_chunks[1]}\n",
    "\n",
    "# Question:\n",
    "# {user_question}\n",
    "\n",
    "# Only return the SQL query.\n",
    "# \"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "afc1021e",
   "metadata": {},
   "outputs": [],
   "source": [
    "context = \"\\n\".join(relevant_chunks)\n",
    "\n",
    "final_prompt = f\"\"\"\n",
    "[INST]You are a helpful SQL assistant. Based on the following schema and user question, write only a SQL query.\n",
    "\n",
    "Schema:\n",
    "{context}\n",
    "\n",
    "Question:\n",
    "{user_question}\n",
    "\n",
    "Return only the SQL query. [/INST]\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f82e1325",
   "metadata": {},
   "source": [
    "# Model\n",
    "\n",
    "Overview\n",
    "Model Name: meta-llama/Llama-2-7b-chat-hf\n",
    "\n",
    "Size: 7 billion parameters (7B)\n",
    "\n",
    "Type: Chat-tuned version of LLaMA 2 (fine-tuned for dialogue)\n",
    "\n",
    "Architecture: Transformer-based decoder-only language model\n",
    "\n",
    "Hosting: Available on Hugging Face (with gated access)\n",
    "\n",
    "License: Meta AI’s custom license – not fully open-source, access requires approval from Meta or Hugging Face gate\n",
    "\n",
    "Primary Use: Natural language generation, conversational AI, coding assistance, summarization, reasoning tasks\n",
    "\n",
    "# Key Features\n",
    "\n",
    "Fine-tuned for Chat: LLaMA-2 base model is further fine-tuned using Reinforcement Learning with Human Feedback (RLHF), similar to ChatGPT\n",
    "\n",
    "Supports role-based conversations: e.g., user/system/assistant prompt formatting\n",
    "\n",
    "Compatible with Hugging Face Transformers and accelerated inference tools like transformers, vLLM, AutoGPTQ, and bitsandbytes\n",
    "\n",
    "Used with FlashAttention2, trust_remote_code=True, and optimized model loading options like device_map=\"auto\" or quantized loading\n",
    "\n",
    "Ideal Use Cases\n",
    "RAG (Retrieval-Augmented Generation)\n",
    "\n",
    "Conversational AI assistants\n",
    "\n",
    "Code completion & explanation\n",
    "\n",
    "Text summarization\n",
    "\n",
    "Knowledge QA systems\n",
    "\n",
    "Educational bots & tutoring systems\n",
    "\n",
    "# Model Specs Summary\n",
    "Feature\tDescription\n",
    "\n",
    "Parameters\t7 billion\n",
    "\n",
    "Max context length\t~4096 tokens\n",
    "\n",
    "Format\tHugging Face Transformers format\n",
    "\n",
    "Special Token Support\tUses [INST] and [/INST] for chat prompts\n",
    "\n",
    "Inference Compatibility\tTransformers, vLLM, HF Text Generation\n",
    "\n",
    "Chat Tuning\tTrained on ~1T tokens and refined with RLHF\n",
    "\n",
    "Base Model\tLlama-2-7b\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38b552cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\srini\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\transformers\\models\\auto\\tokenization_auto.py:809: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\srini\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\transformers\\models\\auto\\auto_factory.py:471: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4c88ef7eeb5c4a8b927a0e52dc775ac2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some parameters are on the meta device because they were offloaded to the disk and cpu.\n"
     ]
    }
   ],
   "source": [
    "# from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
    "\n",
    "# model_id = \"meta-llama/Llama-2-7b-chat-hf\"\n",
    "# hf_token = \"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\"  # use your actual token\n",
    "\n",
    "# tokenizer = AutoTokenizer.from_pretrained(model_id, use_auth_token=hf_token)\n",
    "# model = AutoModelForCausalLM.from_pretrained(model_id, use_auth_token=hf_token, device_map=\"auto\")\n",
    "\n",
    "# llm_pipe = pipeline(\"text-generation\", model=model, tokenizer=tokenizer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "5a99fe90",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f6cc01bfca9b4ee38211c5898a1722b8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some parameters are on the meta device because they were offloaded to the cpu and disk.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
    "\n",
    "model_id = \"defog/sqlcoder-7b-2\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id, trust_remote_code=True)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id, device_map=\"auto\", trust_remote_code=True)\n",
    "\n",
    "pipe = pipeline(\"text-generation\", model=model, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0400b9a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some parameters are on the meta device because they were offloaded to the cpu.\n"
     ]
    }
   ],
   "source": [
    "# from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline,AutoModelForSeq2SeqLM\n",
    "\n",
    "# model_id = \"google/flan-t5-base\"\n",
    "# hf_token = \"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\"  # use your actual token\n",
    "\n",
    "# tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "# model = AutoModelForSeq2SeqLM.from_pretrained(model_id, use_auth_token=hf_token, device_map=\"auto\")\n",
    "# llm_pipe = pipeline(\"text2text-generation\", model=model, tokenizer=tokenizer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "2cd426d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n[INST]You are a helpful SQL assistant. Based on the following schema and user question, write only a SQL query.\\n\\nSchema:\\nTable: orders\\nColumns: order_id (INT), customer_id (INT), order_date (DATE), total_amount (FLOAT)\\nTable: customers\\nColumns: customer_id (INT), name (TEXT), email (TEXT), region (TEXT)\\nTable: reviews\\nColumns: review_id (INT), product_id (INT), customer_id (INT), rating (INT), comment (TEXT)\\nTable: order_items\\nColumns: order_item_id (INT), order_id (INT), product_id (INT), quantity (INT), item_total (FLOAT)\\n\\nQuestion:\\nFor each subscription plan in 2024, identify the top 3 regions with the highest number of active subscribers. For each region, show the average order value, total number of returns, and average product rating across all orders placed during the subscription period. Only consider customers who have not raised any support tickets during their subscription.\\n\\nReturn only the SQL query. [/INST]\\n'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "4fefc8bc",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'llm_pipe' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[35], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[43mllm_pipe\u001b[49m(final_prompt, max_new_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100\u001b[39m, do_sample\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(response[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgenerated_text\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n",
      "\u001b[1;31mNameError\u001b[0m: name 'llm_pipe' is not defined"
     ]
    }
   ],
   "source": [
    "response = llm_pipe(final_prompt, max_new_tokens=100, do_sample=True)\n",
    "print(response[0][\"generated_text\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2be857d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    }
   ],
   "source": [
    "response = pipe(final_prompt, max_new_tokens=100, do_sample=True)\n",
    "print(response[0][\"generated_text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4e94c971",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'generated_text': 'OrderAmount (orders) = (sales) for (order=int) customer and (order_date=int) total_amount (fLOAT) = (order_int) * (order_date-float) + (year = 2024)'}]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb5109f8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84070e9f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caa0efea",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "db645087",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\srini\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\transformers\\models\\auto\\tokenization_auto.py:809: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\srini\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\transformers\\models\\auto\\auto_factory.py:471: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5f982487bb8e45ddacd2afbb55a060d4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some parameters are on the meta device because they were offloaded to the cpu.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
    "\n",
    "model_id = \"meta-llama/Llama-2-7b-chat-hf\"\n",
    "hf_token = \"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\"  # Replace with your real token\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id, use_auth_token=hf_token)\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    use_auth_token=hf_token,\n",
    "    torch_dtype=\"auto\",\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "\n",
    "pipe = pipeline(\"text-generation\", model=model, tokenizer=tokenizer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c9c885e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = \"\"\"\n",
    "Table: orders\n",
    "Columns: order_id (INT), customer_id (INT), order_date (DATE), total_amount (FLOAT)\n",
    "\n",
    "Table: customers\n",
    "Columns: customer_id (INT), customer_name (TEXT), region (TEXT)\n",
    "\"\"\"\n",
    "\n",
    "question = \"Show total sales for each region in 2024.\"\n",
    "\n",
    "prompt = f\"\"\"You are an expert in SQL. Based on the following table schema, generate a valid SQL query.\n",
    "\n",
    "Schema:\n",
    "{schema}\n",
    "\n",
    "Question:\n",
    "{question}\n",
    "\n",
    "Return only the SQL query.\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "17f6a2f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\srini\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\transformers\\models\\llama\\modeling_llama.py:602: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:555.)\n",
      "  attn_output = torch.nn.functional.scaled_dot_product_attention(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are an expert in SQL. Based on the following table schema, generate a valid SQL query.\n",
      "\n",
      "Schema:\n",
      "\n",
      "Table: orders\n",
      "Columns: order_id (INT), customer_id (INT), order_date (DATE), total_amount (FLOAT)\n",
      "\n",
      "Table: customers\n",
      "Columns: customer_id (INT), customer_name (TEXT), region (TEXT)\n",
      "\n",
      "\n",
      "Question:\n",
      "Show total sales for each region in 2024.\n",
      "\n",
      "Return only the SQL query.\n",
      "\n",
      "Hint: You may need to use a join to get the data from both tables.\n"
     ]
    }
   ],
   "source": [
    "response = pipe(prompt, max_new_tokens=200, do_sample=True)\n",
    "print(response[0][\"generated_text\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b63b39a7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9406bf40",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dda8e8c8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
